A brief Literature survey on Knowledge Transfer Models

| Model | Year | Architecture | Datasets and Benchmarks | Advantages and Limitations|
| -------------- | ------- | ---------------------------- | --------------------- | --------------------- |
| **DKT (Deep Knowledge Transfer)** | 2015  | Recurrent Neural Network generates an hidden state with every interaction which represents the students knowledge  | The paper used AUC as the metric <br> The following models are used as the bench mark: BKT. <br> 1. Simulated Data - 0.75 (0.54) <br> 2. KHAN Academy Data - 0.85 (0.68) <br> 3. Asssissments2009 - 0.86 (0.67) | Utilization of RNN to capture the temporal knowledge state of the student <br><br> Lack of interpretibility and inconsistant predictions are the drawbacks. The model also suffers from overfitting problem.|
| **DynamicKey-Value Memory Networks (DKVMN)** | 2017 | The model uses  a static key matrix to store latent KCs and a dynamic matrix called a value matrix to sotre and update the mastery of corresponding KCs through read and write operations over time. | The model uses Synthetic-5, ASSISTments2009, ASSISTments2015, and Statics2011 datasets <br> The performance compared to DKT is as follows: <br> 1. Synthetic5 - 82.73 (80.34) <br> 2. ASSIST2019 - 81.57 (80.53) <br> 3. ASSIST2015 - 72.68 (72.52) <br> 4. Statics2011 - 82.84 (80.20)| The model gives enhanced representation of the students knowledge over different concepts. <br><br> Highly dependent on learning activites ignoring existing knowledge state, Ignorance of Forgetting during prediction and uniform treatment of exercises. |
| **DKT+** | 2018 | The architecture is similar to DKT with additional Regularization term. Three regularization terms - reconstruction error r to resolve the reconstruction problem, and waviness measures w1 and w2 to smoothen the predicted knowledge state transition. <br> $L' = L + \lambda_r r + \lambda_{w_1} w_1 + \lambda_{w_2} w_2^2$ |  The model has been trianed on ASSIST2009, ASSIST2015, ASSISTChall, Statitics2011, Simulated-5. <br><br> The models performance was evaluated against DKT. | The paper was able to solve the reconstruction and inconsistant problems of DKT. |
| **Deep - Item Response Theory (Deep-IRT)** | 2019 | The model is a combination of Item Response Theory (IRT) and the DKVMN model. The DKVMN process student's learning  to estimate the hidden state, and these estimates are then passed to IRT model to calculate the probability. The model is very similar to the DKVMN with an addition of IRT function before the model prediction. The IRT fuction is the 1 parameter logistic model also called as Rasch model. By integrating this function the model also considers the item difficulties over the time. | The model is tested on ASSIST2019, ASSIST2015, Statistics2011, Synthetic and FSAI-F1toF3 datasets against the base line models DKT and DKVMN. <br> The results are as follows: <br> 1. ASSIST2009 - 81.65 (81.56) <br> 2. ASSIST2015 - 72.88 (72.94) <br> 3. Statics2011 - 83.09 (83.17) <br> 4. Synthetic - 82.98 (82.97) <br> 5. FSAI-F1toF3 - 68.69 (69.42)| The model may not be the best performing, but was able to reach the levels of SOTA models with improved interpretability. <br><br> The model faces reconstruction issues and assumes that items with same skills are equivalent. |
| **Self-Attentive Knowledge Tracing (SAKT)** | 2019 | The model employs Self-Attention based approach similar to transformer architecture to identify the relavent KCs from the past interactions to estimate the mastery of the KCs to predict the mastery of the current KCs. | The model was trained on the following datasets: Synthetic, ASSIST2009, ASSIST2015, ASSISTChall, Statitics2011. <br> The model was tested against DKT, DKT+ and DKVMN. <br> The results are as follows: <br> 1. Synthetic - 0.832 (0.24) <br> 2. ASSIST2009 - 0.848 (0.822) <br> 3. ASSIST2015 - 0.854 (0.737) <br> 4. ASSISTChall - 0.734 (0.734) <br> 5. Statitics2011 - 0.853 (0.835)| The model was abled to identify the relavent KCs, parllel processing, was abled to work better with sparse data. <br><br> Even though it is better than previous models, Data Sparsity is great challenge as the model relies on past interactions. |
| **Graph-based Knowledge Tracing (GKT)** | 2019 | The model was inspired from the Grpah Neural Networks (GNN), due to the naturally exisiting graph structure relationships between the KCs. The model is similar to GNN, consisting of 3 modules: Aggregate module responsible for aggregating the knowledge between the related KCs and the exercises, Update module for updating the temporal knowledge state and the Predict module predicting the responses based on the hidden state. | The model has been tested on ASSSIT2019, KDDCup. <br> The models used for the baseline are DKT and DKVMN. <br> The results are as follows: <br> 1. ASSIST2019 - 0.723 (0.710) <br> 2. KDDCup - 0.769 (0.753) | The model has given an new perspective of visualizing the domain and effectively representing the KCs and Student Knowledge states. <br><br> The architecture of the model makes the computation complex and diffivult to interpret. |
| **Separated Self-Attentive Neural Knowledge Tracing (SAINT)** | 2020 | The model is an encoder-decoder architecture, which seperates the entry of the exercises and the response embedding sequences, unlike the earlier SAKT model. The encoder applies the self-attention for exercise embeddings and the decoder applies the self-Attention for the response embeddings. | The model is trained using EdNet dataset. The baseline models used are SAKT, SSAKT etc. The model showed an accuracy of 0.7811 (0.7777) | The model was able to provide a better performance compared to previous models. <br><br> Many of the challenges of the previous methods were not solved, and the model was missing a through evaluation on standard datasets and against SOTA baseline models. |
| **Attentive knowledge tracing (AKT)** | 2020 | The model uses an attentention mechanism to relate the past responses using exponential decay and context-aware relative distance. The model utilizes Rasch model to capture individual differences among questions on the same concept. | The model was trained on ASSIST2009, ASSIST2015, ASSIST2017, Statitics2011. <br> The following models are used as baseline DAT, DKT+, DKVMN, SAKT. <br> The results are as follows: <br> 1. Statistics2011 - 0.8265 (0.8301) <br> 2. ASSIST2009 - 0.8346 (0.817) <br> 3. ASSIST2015 - 0.7828 (0.7313) <br> 4. ASSSIT2017 - 0.7702 (0.7282) | The model captures the context-awareness of questions and KCs, effective handling of repeated attempts and enhanced accuracy. <br><br> The model suffers from Data Sparsity Challenges and Limited Interpretability. |
| **Adversarial training based KT method (ATKT)** | 2021 | The model uses attention based LSTM which apply adversial pertubations into original student interactions sequence.| The model has been trained on Statics2011, ASSIST2009, ASSIST2015 and ASSIST2017. <br> The following models have been used as the baseline BKT+, DKT, DKVMN, SAKT, AKT. <br> The results are as follows: <br> 1. Statics2011 - 0.8325 (0.8301) <br> 2. ASSIST2009 - 0.8244 (0.8170) <br> 3. ASSIST2015 - 0.8045 (0.7828) <br> 4. ASSIST2017 - 0.7297 (0.7282)| The model was abled to solve the issue of overfitting, thus improving the generalizatiion and enhanced results. <br><br> |
| **Individual Estimation Knowledge Tracing (IEKT)** | 2021 | IEKT architecture contains a read and write stages in the RNN Cell, But to include the congnition levels, A Cognition Estimation (CE) module designed to estimate students cognition level is added in the read module and a Knolwdge Acquisition Sensitivity Estimation (KASE) designed to estimate the students knowledge acquisition sensitity while udpating the knowledge states is added in the write module. | The datasets used are ASSIST09, ASSIST12, EdNET and Junyi. <br> The baseline models used are: DKT, DKVMN, GKT, CKT, SAKT, AKT etc <br> The results are: <br> 1. ASSIST2009 - 0.7720 (0.7544) <br> 2. ASSIST2012 - 0.7341 (0.7098) <br> 3. EdNet - 0.7305 (0.7087) <br> 4. Junyi - 0.8999 (0.8939)| IEKT accounts the students cognition levels and the knowledge acquisition sensitivity are taken into account. The model has achieved SOTA performance. <br><br> There is a risk of overfitting for the model, the model will be computationally complex as it estimates the parameters for each student individuality, which leads to scalability issues. |
| **simpleKT**| 2022 | The model uses a very simple architecture rather than the sophesticated architectures. Initially, inspired by Rasch model, a quetion-specific difficulty vector is used to capture the individual differeences among questions on the same KC. Later ordinary dot-product attention function is used to extract knowledge states from students past learning history. Finally A tow layer fully connect network is used for the prediction. | The datasets used are ASSIST2019, Algebra2005, Bridge2006, NIPS34, Statics2011, ASSIST2015, POJ. <br> The baseline models considered are: DKT, DKT+, DKT-F, KQN, LPKT, IEKT, DKVMN, ATKT, GKT, SAKT, SAINT, AKT. <br> The results are as following: <br> 1. AS2009 - 0.7744 (0.7861) <br> 2. AL2005 - 0.8254 (0.8416) <br> 3. BD2006 - 0.8160 (0.8208) <br> 4. NIPS34 - 0.8035 (0.8045) <br> 5. Statics2011 - 0.8199 (0.8309) <br> 6. AS2015 - 0.7248 (0.7285) <br> 7. POJ - 0.6252 (0.6281) | The model is simple but one of the best performing models. <br> <br> The model may not capture intricate patterns because of its simplicity and may lead to potential overfitting because of inadequate regularization. |
| **DTransformer** | 2023 |  The model initially estimates question-level mastery by employing Temporal and Cumulative Attention (TCA), by including the temporal term from the AKT model and the Maxout operation in the attention to capture the cummulative effect and then the output is used to estimate the knowledge level mastery by multi-head TCA. The model adds a new techniques in the model pipeline called Contrastive loss of knowledge states: which employs flip response, drop itme and the swapping adjacent items to preserve contrastive learning. Finally these knowledge states are used to predict for next question. | The model has been trained on ASSIST09, ASSIST17, ALGEBRA05, Statics2011. <br> The baseline models used are DKT, DKT+, DKVMN, SAKT, AKT, CL4KT. <br> The results are as following: <br> 1. ASSIST09 - 0.8146 (0.7891) <br> 2. ASSIST17 - 0.7506 (0.7464) <br> 3. ALGEBRA05 - 0.7946 (0.7891) <br> 4. STATICS - 0.8382 (0.8299) | The model brings new perspective of capturing evolving knowledge states rather than focusing more on next question prediction. The model brings stability in knowledge states and preserves contrastive learning. |
| **A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model (SINKT)** | 2024 | The model first encodes the textual information using an pre-trained LLM to model the complex relationships of concepts and questions in the graph structure. Later it encodes the sematic information of the graph using 3 Graph Attention Networks (GATs) for concept-concept, concept-question and question-concept relationships and then passed a GAT layer is used to average the reponses of all the outputs and sent to an GRU cell for final prediction. | The model is evaluated on 4 daetasets: ASSIST2019, ASSIST12, Junyi and Programming. <br> The baseline models considered are DKT, DHKT, DKVMN, SKVMN, CKT, SAKT, EERNNA, AKT, GKT, SKT, IEKT, LBKT. <br> The results are as follows: <br> 1. ASSIST09 - 0.7726 (0.7511) <br> 2. ASSIST12 - 0.7028 (0.6918) <br> 3. Junyi - 0.8095 (0.8018) <br> 4. Programming - 0.8267 (0.8261)| The model was the first to leverage the textual and structural information of the concepts and questions, rather than just the ID based correlation between them. The model posses Inductive capability, predicting responses to new questions which are not in training, thus solving the cold-start problem and enhanced perfomance. <br><br> The model is highly computationally expensive and the preformance is dependent on pretrained models. |
| **Concept map-driven Response disentanglement method for enhancing Knowledge Tracing (CRKT)** | 2024 | First the model process the sekected and unchoosed responses using the response encoder then it is passed to attention-based knowledge retriever along with the question representations.  The representations are inetgrated with concept map with question-specific edge weights, thus deducing the knowledge state of each concept using GNN. Finally IRT is used for predictions. | The model uses Poly, DBE-KT22, EdNet, NIPS34, ENEM datasets. <br> The baseline models used are DKT, GKT, SAKT, AKT, CL4KT, DTransformer, DP-DKT. <br> The results are as follows: <br> 1. Poly - 82.72 (81.12) <br> 2. DBE-KT22 - 81.04 (80.07) <br> 3. EdNet - 73.22 (72.18) <br> 4. NIPS34 - 77.27 (77.19) <br> 5. ENEM - 74.70 (74.29)| Incorporation of concept maps and representing knowledge at concept level, Disentanglement of the response, thus providing deeper insights from unchosen responses.|
| **Revisiting Knowledge Tracing (ReKT)** | 2024 | The model captures the knowledge state from three distinct perspective questions, concepts and domains. Then the representations are embedded using the FRU unit (Forget-Response-Update) which composes of two linear regression units. These knowledge states along with the interaction history is passed to a Linear unit for the prediction. | The datasets used are ASSIST09, ASSIST12, ASSIST15, ASSIST17, Statics2011, EdNet, Eedi. <br> The baselines used are DKT, DKVMN, DeepIRT, DKT+, SAKT, AKT, GKT, SAINT, ATKT. <br> The results are as follows: <br> 1. ASSIST09 - 0.7340 (0.7337) <br> 2. ASSIST12 - 0.7385 (0.7390) <br> 3. ASSIST15 - 0.7624 (0.7607) <br> 4. ASSIST17 - 0.6690 (0.6699) <br> 5. Statics2011 - 0.8319 (0.8275) <br> 6. EdNet - 0.7153 (0.7028) <br> 7. Eedi - 0.7208 (0.7195)| The model uses a simple architecture yet but was able to give the best results, It was able to represent knowledge from 3 different perspectives. |
| **Uncertainity-aware Knowledge Tracing (UKT)** | 2025 | The model takes two-types of uncertainties into account: Epistemic Uncertainity - assess a student's true knowledge level reflecting congnitive differences between students and Aleatory Uncertainity - factors careless errors or lucky guesses. The model uses a stochastic embedding layer as a gaussian distribution with mean and covariance representing students initial knowledge and uncertainity in learning process. The Wasserstein based self attention layer is used to anlayze the gaussian distribution to track changes in knowledge states. Then a FFN is used to assess the students knowledge states. To address aleatory uncertainity, and aleatory uncertainity aware contrastive learnig layer is used. Finally the adjusted uncertainity with student's knowledge level is used to predict. | The Datasets used are ASSIST2009, Algebra2005, Bridge2006, NIPS34, ASSIST2015, POJ. <br> The baseline models used are DKT, SAKT, SAINT, ATKT, AKT, SimpleKT. <br> The results are as follows: <br> 1. AS2009 - 0.8563 (0.8474) <br> 2. AL2005 - 0.9320 (0.9294) <br> 3. BD2006 - 0.8178 (0.8167) <br> 4. NIPS34 - 0.8035 (0.7966) <br> 5. AS2015 - 0.7267 (0.7282) <br> 6. POJ - 0.6301 (0.6218) | The model incorporates epistemic and aleatory uncertainities thys capturing broader spectrum of student learning behavior. |
| **LLM-KT** | 2025 | The core idea is to align LLMs with KT through a Plug-and-Play Instruction mechanism, enabling the integration of multiple data modalities and improving the model's reasoning abilities. The Plug and Play component generates the embeddings for questions and concepts for LLM usage. Then Two modules are used Plug-in Context which  captures the semantic information of questions and concepts using a context encoder (ex BERT) and the Plug-in Sequence Module to integrate the sequential interaction patterns learned from traditional KT models into the LLM are used. | The datasets used are: Assist2009, Assist2015, Junyi, and NIPS2020. The model results are compared with 20 models: DKVMN, SAKT, AKT, LPKT, LBKT, AT-DKT, MRT-KT, BiDKT, MLFBK, LBKT, DCL4KT-A, EERNN, EKT, RKT, LLM-FT(ID), LLM-FT(TokenID), LLM-FT(Text), GPT-4o. <br> The results are as follows: <br> 1. Assist09 - 0.8870(0.8524) <br> 2. Assist15 - 0.9356 (0.9092) <br> 3. 0.9018 (0.8948) <br> 4. 0.8291 (0.7890)| The model effectively combines the reasoning power of LLMs with the sequential modeling strengths of traditional KT methods.|
|**CSKT**|||||
|**StableKT**|||||
|**BiDKT**|||||
|**Exercise hierarchical feature enhanced knowledge tracing**|||||
|**MLFBK**|||||
|**LBKT**|||||
|**HawkesKT**|||||
|**LPKT**|||||
|**Progressive knowledge tracing**|||||
|**PEBG**|||||
|**DGEKT**|||||
|**Exercise hierarchical feature enhanced knowledge tracing**|||||
|**RKT**|||||
|**EERNN**|||||
|**EKT**|||||
|**Difficulty-Focused Contrastive Learning for Knowledge Tracing with a Large Language Model-Based Difficulty Prediction**|||||



### Observations and questions
1. Graph based models may not be more effective than the attention based models (because the attention is able to understand the internal relationships)
2. Simple architecture are very effective rather than complex models and can be at the level of SOTA models.
3. The models are not still not very good in terms of accuracy, as it is much complex problem.
4. There is a need for Overall and current modelling of the user performance -> Learn from corrects and mistakes.
5. The models are highly supervised for the concept and question mapping? can we optimize it? if how?
6. Multi modality? is it able to perform for all tasks.


### Major Challaneges faced by current models
1. **Multi-tagging**: The models doesn't support multi-tagging, if it supports, it doesn't give importance to the multiple tags and their relanvence and their state changes at the time of back propogation is the question?
2. **Cold-start problem**: Many llm based models pass the student's previous interactions and their responses for those questions as the input. This leads to the cold-start problem for the new students
3. **Insufficient modelling of students cognitive behaviour or state**: students cognitive behaviour place a main role in learning, models fail to incorporate such features in the model.
4. **DIfficulty ratings**: many Models arent considering the difficulty of the problems.
5. **Handling longer student interactions**: The LLMs might face issue with the prompt lengths when incorporating when longer student interactions are considered
6. **Interpretability**: The major concern with the Deep learning KT models is the interpretability. No one has no idea on the internal states of the model and the concept level understanding of the student is much difficult to interpret.
7. **Multi-Modality**: Long lasting problem with the KT models is the multi-modality of these models, they aren't able to handle diverse questions.
8. **Cost**: LLM powered models are very costly.